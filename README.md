# Optimisation-and-the-Gradient-Descent-Algorithm

Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent in machine learning is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible.
Gradient descent is an optimization algorithm that’s used when training a machine learning model. It’s based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.
To start by defining the initial parameter’s values and from there the gradient descent algorithm uses calculus to iteratively adjust the values so they minimize the given cost-function. 
